{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beofre carrying out any work, I import my required libraries, and load my data into a pandas DataFrame:\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import prince\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fde303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I load the dataset into a panda data frame:\n",
    "data= pd.read_csv('/Users/jenniferfortuny/02450_project/2023-09-15_jennifer_pca_section/2023-09-08_jennifer_filtered_complete_copy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd460f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Data Visualisation:\n",
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "1. Check for outliers.\n",
    "For the continuous attributes: age, edu-num, hour-per-week.\n",
    "I make a histogram with a boxplot.\n",
    "\n",
    "For the categorical attributes: workclass, occupation.\n",
    "I only make a histogram.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb657ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I begin by splitting my attributes into two lists:\n",
    "continuous_attributes = data.columns[:3]\n",
    "categorical_attributes = data.columns[3:]\n",
    "# I use one-hot encoding to encode categorical attributes\n",
    "categorical_encoded = pd.get_dummies(data, columns=categorical_attributes, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f731b2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# I plot the histogram with box plot for each of the three continous attributes:\n",
    "for column in continuous_attributes:\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create grid\n",
    "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1])\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "\n",
    "    # Histogram on the top axis (ax1)\n",
    "    sns.histplot(data[column], kde = True, ax = ax1)\n",
    "    ax1.set_title(f'Histogram with Boxplot for {column}')\n",
    "    ax1.set_xlabel('') # This is to keep the x-axis label empty on the top\n",
    "    ax1.set_ylabel('Frequency')\n",
    "\n",
    "    # Box plot on the bottom axis (ax2)\n",
    "    sns.boxplot(x = data[column], ax = ax2)\n",
    "    ax2.set_xlabel(column)\n",
    "\n",
    "    # I find and label the outliers on the box plot\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = data[(data[column] < (Q1 - 1.5 * IQR))\n",
    "                                   | (data[column] > (Q3 + 1.5 * IQR))][column]\n",
    "    for outlier in outliers:\n",
    "        ax2.text(outlier, -0.18, f'{outlier:.0f}', ha='center', va='top', fontsize=8, color='blue')\n",
    "        # -0.02 places my text at y = -0.02, i.e. below the dot on the box plot.\n",
    "    plt.tight_layout()\n",
    "    ax2.set_ylabel(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ed409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I plot the histogram-only plots for each of the three continous attributes:\n",
    "for column in categorical_attributes:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data[column], kde = True)\n",
    "    plt.title(f'Histogram for {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    # I angle the x-axis labels a bit to show all the words clearly\n",
    "    plt.xticks(rotation=20, ha='right', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec6375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Check for outliers.\n",
    "# To check if the attributes are normally distributes,\n",
    "# I will begin by reflectiong on the results of the histograms:\n",
    "# continous attributes:\n",
    "#     age: tail to the right, so right sqewed distribution.\n",
    "#     edu-num: looks like a bimodial distribution.\n",
    "#     hours-per-week: looks like an extreme plot with on highly frequent value at 35-40.\n",
    "# categorical attributes:\n",
    "#    workclass: extreme with \"Private\" at the highest frequency.\n",
    "#    occupation: some outliers, most seem to be at the similar frequency, no clear trend.\n",
    "\n",
    "# I will use Q-Q plots to determin if they attributes have a formal normal distribution.\n",
    "# If the data are mostly on the y=x line in the Q-Q plot, then we can assume there is a normal distribution.\n",
    "# I will plot the continous variables' Q-Q plots:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will rename the previous variable:\n",
    "for column in continuous_attributes:\n",
    "    # Since I am also using statsmodels now, in addition to matplotlib.\n",
    "    # Here I create a figure and axis just for the Q-Q plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sm.qqplot(data[column].dropna(), line = '45', fit = True)\n",
    "    plt.title(f'Q-Q Plot for {column}')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\"\"\"\n",
    "Notes:\n",
    "The age attribute shows a U-shape pattern.\n",
    "The points fall below the y=x line at the lower end, and above the line at the higher end.\n",
    "This suggests we have fewere extreme values than we would expect compared to a perfect normal distribution\n",
    "i.e. it is a lighter-tailed than a normal distribution.\n",
    "Therefore we have a somewhat uniform distribution, what is more concentrated around the median, less so towards the tails.\n",
    "This means our data could have a wide range of ages, \n",
    "and when compared to a normal distribution -  as many individuals who are very young or very old.\n",
    "This could be becasue the data is focused on working age adults, which would include less people who are very young or very old.\n",
    "The right skew in the histogram suggests relatively less older individuals than younger ones.\n",
    "\n",
    "The edu-num distribution shows a shape similar to w, this suggests a binomial distribution, in agreement with the histogram.\n",
    "This suggests the data has two major distributions of education levels.\n",
    "\n",
    "The hours-per-week Q-Q plot shows a sharp incline, a long flat section, then another sharp incline.\n",
    "The flat section is the major accumulation of the data, with coincides with the histogram.\n",
    "The large portion of the dataset are people who work standard full-time hours.\n",
    "This is a non-linear Q-Q plot, so it does NOT show a normal distribution.\n",
    "\n",
    "For the categorical variables, I don't expect a normal distribution.\n",
    "It might be more valuable to understand the frequency and mode of these attributes.\n",
    "From the observations of the histogram:\n",
    "The workclass attributes prodominatly represents the private sector.\n",
    "The occpupation attribute shows a more uniform distribution, with some roles acting as outliers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8022d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Check correlations between variables.\n",
    "I want to visualise this continous with a heatmap, using a correlation matrix.\n",
    "This will help show the relationships between the continous variables.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f68e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First a calculate the correlation matrix:\n",
    "corr = data[continuous_attributes].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d18675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then I make a heatmap:\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot = True, cmap = 'coolwarm', vmin = -1, vmax = 1)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34df917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Carry-out the PCA analysis:\n",
    "\n",
    "If your attributes have different scales you,\n",
    "should include the step where the data is standardizes\n",
    "by the standard deviation prior to the PCA analysis.\n",
    "\n",
    "1. The amount of variation explained as a function of the number of PCA components included.\n",
    "2. The principal direction of the considered PCA components (either find a way to plot them or interpret them in terms of the features).\n",
    "3. The data projected onto the considered principal components.\n",
    "\n",
    "\"\"\"\n",
    "# First I will take somes steps to prepare the data for visualisation and clustring.\n",
    "# I will put the continous and encoded categorical attributes together\n",
    "final_data = pd.concat([data[continuous_attributes], categorical_encoded], axis = 1)\n",
    "print(final_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then I will standarise the data\n",
    "scaler = StandardScaler()\n",
    "data_standarized = scaler.fit_transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2611b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now I wil carry out the PCA analysis.\n",
    "I will start by running the PCA without initially setting a number of components.\n",
    "This will help me understand the total explained variance for each component.\n",
    "It will also be helpful in determining the number of PCs that would capture most of the dataset's variance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA applied without specifying the number of components.\n",
    "pca = PCA()\n",
    "principal_components_full = pca.fit_transform(data_standarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, I will look into the explained variance.\n",
    "# I will plot the explained variance ratio, to see how much variance each component explains.\n",
    "# This will help me determin a good number of PCs.\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will check for the first n components that explain 95% or more of the variance.\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(cumulative_variance, marker='o', linestyle='--', color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf920ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will annotate the PCAs on the plot\n",
    "# It seems like the first 20 to 22 PCs explain over 95% of the variance in the data.\n",
    "for i in range(len(cumulative_variance)):\n",
    "    plt.annotate(f\"PC{i+1}: {cumulative_variance[i] * 100:.2f}%\",\n",
    "                 (i, cumulative_variance[i]),\n",
    "                 textcoords=\"offset points\",\n",
    "                 xytext = (0, 10),\n",
    "                 ha = 'center',\n",
    "                 fontsize = 9,\n",
    "                 color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumlative Explained Variance')\n",
    "plt.title('Cumlative Explained Variance as Number of Components Increases')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4eb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given that we are working with a large number of potential PCs\n",
    "# I will carry out some dimensional reduction, \n",
    "# and only use three PCs for the PCA analysis and visualisation.\n",
    "# I will also try to retain as much information about the data as possible.\n",
    "# To confirm the results from the graph:\n",
    "\"\"\"\n",
    "print(f\"Cumulative variance explained by first 20 components: {cumulative_variance[19]*100:.2f}%\")\n",
    "print(f\"Cumulative variance explained by first 21 components: {cumulative_variance[20]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ab0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21 PCs is a lot of PCs. This is most likely the result of having used one-hot encoding for our categorical data.\n",
    "# I want to use much fewer PCs but:\n",
    "print(f\"Cumulative variance explained by first 3 components: {cumulative_variance[3]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2420280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Given this situation. I would like to apply the PCA only on to the continous part, and the MCA on the categorical part.\n",
    "# I will first apply MCA to the categorical attributes.\n",
    "\"\"\"\n",
    "# I will initialise MCA with the prince module and fit the encoded categorical data:\n",
    "# To begin, I will not specify the number of components\n",
    "mca = prince.MCA()\n",
    "mca = mca.fit(categorical_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I will transform the categorical data\n",
    "mca_coordinates = mca.transform(categorical_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c658fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, I will take a look at MCA's explained inertia for each component,\n",
    "# which is similar to PCA's explained variance.\n",
    "# I will start by getting the eigenvalues:\n",
    "eigenvalues = mca.eigenvalues_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca821776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total inertia:\n",
    "total_inertia = sum(eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89097bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the proportion of explained inertia:\n",
    "explained_inertia = [eig/total_inertia for eig in eigenvalues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f6fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MCA's result shows that there are {len(explained_inertia)} PCs for our categorical data:\")\n",
    "for i, inertia in enumerate(explained_inertia, start=1):\n",
    "    print(f\"PC{i}: {inertia*100:.2f}% of the variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b49df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I will plot the explained inertia:\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(explained_inertia, marker = 'o', linestyle = '--', color = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotating the points\n",
    "for i, inertia in enumerate(explained_inertia):\n",
    "    plt.annotate(f\"PC{i+1}: {inertia*100:.2f}%\", (i, inertia), \n",
    "                 textcoords=\"offset points\", xytext=(0, 10), ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a548f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Inertia')\n",
    "plt.title('Explained Inertia as Number of Components Increases')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beaca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create a 2D scatter plot for the categorical values.\n",
    "# First I extract coordinates for the first two components:\n",
    "x = mca_coordinates[0]\n",
    "y = mca_coordinates[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.scatter(x, y, edgecolor=\"k\", color=\"blue\", alpha=0.6)\n",
    "plt.title(\"MCA Scatter Plot for Individuals\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445249ab",
   "metadata": {},
   "source": [
    "Noting the result form this MCA analysis on the categorical data\n",
    "There 2 PCs explain all the variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I will move on to apply PCA on the continous data.\n",
    "# First I use the column names to extract the continous data, i.e. no column names.\n",
    "continous_data = data[continuous_attributes]\n",
    "# Then I standarise the data:\n",
    "continous_data_standarized = scaler.fit_transform(continous_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e73874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begining without specifying the number of components used.\n",
    "pca = PCA()\n",
    "pca_coordinates = pca.fit_transform(continous_data_standarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I will take a loko at the explained variance of the continous columns:\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"There are {len(explained_variance)} PCs in total for the countinous attributes.\")\n",
    "print(f\"Explained variance for each PC are: {explained_variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I make a plot to take a look at how much variance each PC explains\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(explained_variance, marker='o', linestyle='--', color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I annotate the PCs on the plot\n",
    "for i, var in enumerate(explained_variance):\n",
    "    plt.annotate(f\"PC{i+1}: {var:.2f}\", (i, var),\n",
    "                 textcoords = \"offset points\", xytext = (0, 10), ha = 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b457d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.title(\"Explained Variance as Number of Components Increases\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b770591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will also make a plot to take a look at the cumulative explained variance.\n",
    "cumulative_variance = np.cumsum(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(cumulative_variance, marker = 'o', linestyle = '--', color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2203ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I annotate the PC on the cumulative variance plot\n",
    "for i, cum_var in enumerate(cumulative_variance):\n",
    "    plt.annotate(f\"PC{i+1}: {cum_var:.2f}\", (i, cum_var),\n",
    "                 textcoords=\"offset points\", xytext = (0, 10), ha = 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087465af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance as Number of Components Increases')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create a 3D scatter plot for the continous columns.\n",
    "fig = plt.figure(figsize = (10, 7))\n",
    "ax = fig.add_subplot(111, projection = '3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093bd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will isolate the coordinates of the first 3PCs.\n",
    "x = pca_coordinates[:, 0]\n",
    "y = pca_coordinates[:, 1]\n",
    "z = pca_coordinates[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a35baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(x, y, z, c=\"b\", marker = \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5fb82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('3D Scatter plot of the the PCs for continous data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf2dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
